from dataclass import dataclass, field

from lerobot.configs.policies import PreTrainedConfig
from lerobot.configs.types import FeatureType, NormalizationMode, PolicyFeature
from lerobot.optim.optimizers import AdamWconfig
from lerobot.optim.schedulers import ConsineDecayWithWarmSchedulerConfig
from lerobot.policies.rtc.configuration_rtc import RTCConfig
from lerobot.utils.constants import ACTION, OBS_IMAGES, OBS_STATE

DEFAULT_IMAGE_SIZE = 224

@PreTrainedConfig.register_subclass("pi05") #æŠŠä¸‹é¢è¿™ä¸ª PI05Config ç±»æ³¨å†Œè¿› Transformers çš„é…ç½®ç³»ç»Ÿé‡Œï¼Œæ³¨å†Œåå« "pi05"
@dataclass #è‡ªåŠ¨ç”Ÿæˆ __init__() æž„é€ å‡½æ•°ï¼Œæ ¹æ®å†™çš„å­—æ®µï¼ˆå¸¦ç±»åž‹å’Œé»˜è®¤å€¼ï¼‰è‡ªåŠ¨åˆå§‹åŒ–
class PI05Config(PreTrainedConfig):
    paligemma_variant: str = "gemma_2b"
    action_expert_variant: str = "gemma_300m"
    dtype: str = "float32" # Options: "bfloat16", "float32"

    n_obs_steps: int = 1 # æ¯æ¬¡å†³ç­–è¾“å…¥çš„è§‚æµ‹å¸§æ•°
    chunk_size: int = 50 # action_horizon, é¢„æµ‹çš„action stepsæ•°é‡
    n_action_steps: int = 50 # Number of action steps to execute

    # Shorter state and action vectors will be padded to these dimensions
    max_state_dim: int = 32
    max_action_dim: int = 32

    # Flow matching parameters
    num_inference_steps: int = 10 # åŽ»å™ªè¿­ä»£æ­¥æ•°

    # éœ€è¦å¯¹æ—¶é—´æ­¥ t è¿›è¡Œé‡‡æ ·ï¼Œt ~ Beta(Î±,Î²)ï¼Œð›¼=1.5, ð›½=1.0ä¼šå¯¼è‡´åˆ†å¸ƒæ›´åŠ åå‘äºŽ1(çº¯å™ªå£°)
    time_sampling_beta_alpha: float = 1.5
    time_sampling_beta_beta: float = 1.0
    # éœ€è¦æŠŠé‡‡æ ·å‡ºæ¥çš„ t æ˜ å°„åˆ°(0.001ï¼Œ1.0)ï¼Œé˜²æ­¢å‡ºçŽ°ä¸€å¼€å§‹å°±é‡‡æ ·åˆ° t â‰ˆ 0 çš„æƒ…å†µï¼Œt' = offset + scale * t
    time_sampling_scale: float = 0.999
    time_sampling_offset: float = 0.001

    # æœ€çŸ­å‘¨æœŸå’Œæœ€é•¿å‘¨æœŸï¼Œåœ¨åŽé¢å¯¹ timestep åš embedding çš„æ—¶å€™å‘æŒ¥ä½œç”¨
    min_period: float = 4e-3
    max_period: float = 4.0

    rtc_config: RTCConfig | None = None

    image_resolution: tuple[int, int] = (
        DEFAULT_IMAGE_SIZE,
        DEFAULT_IMAGE_SIZE,
    )

    empty_cameras: int = 0

    tokenizer_max_length: int = 200

    ''' 
    ç»™ä¸åŒç±»åž‹çš„æ•°æ®æŒ‡å®šä¸åŒçš„å½’ä¸€åŒ–æ–¹å¼ï¼š
    å›¾åƒï¼šä¸åšå½’ä¸€åŒ–
    STATEå’ŒAction: åˆ†ä½æ•°å½’ä¸€åŒ–ã€‚å› ä¸ºä¸åŒç»´åº¦é‡çº²å·®è·å¾ˆå¤§ï¼ŒåŒæ—¶è¿è¡Œæ—¶å¯èƒ½å­˜åœ¨å¼‚å¸¸æŠ–åŠ¨
    å¯¹æ¯ä¸€ç»´æ‰¾åˆ†åˆ«ç»Ÿè®¡ q_low(æ¯”å¦‚1%åˆ†ä½æ•°) å’Œ q_high(æ¯”å¦‚99%åˆ†ä½æ•°)
    ç„¶åŽæŠŠä¸­é—´çš„éƒ¨åˆ†æ˜ å°„åˆ°ç¨³å®šåŒºé—´: x_norm = 2 Ã— (x - q_low) / (q_high - q_low) - 1
    å¯¹äºŽé‚£2%çš„æ•°æ®, éœ€è¦ä½¿ç”¨clipæˆªæ–­: x_norm = clip(x_norm, -1, 1)
    '''
    normalization_mapping: dict{str, NormalizationMode} = field(
        default_factory = lambda: {
            "VISUAL": NormalizationMode.IDENTITY,
            "STATE": NormalizationMode.QUANTILES,
            "ACTION": NormalizationMode.QUANTILES,
        }
    )

    # Training settings
    '''
    gradient_checkpointing: æ¢¯åº¦æ£€æŸ¥ç‚¹, é€‰æ‹©æ€§ä¸ä¿å­˜æŸäº›ä¸­é—´æ¿€æ´», åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ä¸€æ¬¡forwardæ¢å–æ˜¾å­˜
    compile: æ˜¯å¦å¯ç”¨ torch.compile åŠ é€Ÿ, å®ƒä¼šæŠŠæ¨¡åž‹çš„ forward è¿‡ç¨‹åšå›¾ä¼˜åŒ–å’Œ kernel fusion, max_autotuneæ˜¯å…¶æœ€æ¿€è¿›çš„æ€§èƒ½ä¼˜åŒ–æ¨¡åž‹ã€‚
    compileè™½ç„¶å¯ä»¥å¸¦æ¥åŠ é€Ÿ, ä½†æ˜¯ä¸é€‚åˆåŠ¨æ€shape(éœ€è¦åå¤ç¼–è¯‘, é€Ÿåº¦å¾ˆæ…¢)
    '''
    gradient_checkpointing: bool = False
    compile_model: bool = False
    compile_mode: str = "max_autotune"
    device: str | None = None

    # Finetuning settings
    freeze_vision_encoder: bool = False
    train_expert_only: bool = False

    # optimizer settings:
    optimizer_lr: float = 2.5e-5
    optimizer_betas: tuple[float, float] = (0.9, 0.95)
    optimizer_eps: float = 1e-8
    optimizer_weight_decay: float = 0.01
    optimizer_grad_clip_norm: float = 1.0

    scheduler_warmup_steps: int = 1_000
    scheduler_decay_steps: int = 30_000
    scheduler_decay_lr: float = 2.5e-6

    ''' 
    @dataclass ä¼šå…ˆè‡ªåŠ¨ç”Ÿæˆ __init__() æŠŠæ‰€æœ‰å­—æ®µèµ‹å€¼å®Œï¼Œç„¶åŽç«‹åˆ»è‡ªåŠ¨è°ƒç”¨ __post_init__() è®©ä½ åšé¢å¤–å¤„ç†ã€‚
    å› ä¸º dataclass çš„å­—æ®µèµ‹å€¼æ˜¯â€œå‚»ç“œå¼â€çš„ï¼šåªè´Ÿè´£æŠŠå‚æ•°å­˜è¿›åŽ»ï¼Œä¸ä¼šå¸®ä½ åšæ ¡éªŒã€æŽ¨å¯¼å­—æ®µã€è‡ªåŠ¨ä¿®æ­£ç­‰é€»è¾‘ã€‚
    '''
    def __post_init__(self):
        super().__post_init__()

        if self.n_action_steps > self.chunk_size:
            raise ValueError(
                f"n_action_steps ({self.n_action_steps}) cannot be greater than chunk_size ({self.chunk_size})"
            )
        
        if self.paligemma_variant not in ["gemma_300m", "gemma_2b"]:
            raise ValueError(f"Invalid paligemma_variant: {self.paligemma_variant}")

        if self.action_expert_variant not in ["gemma_300m", "gemma_2b"]:
            raise ValueError(f"Invalid action_expert_variant: {self.action_expert_variant}")

        if self.dtype not in ["bfloat16", "float32"]:
            raise ValueError(f"Invalid dtype: {self.dtype}")
    
    '''
    ç¡®ä¿è¾“å…¥è§„èŒƒä¸€è‡´
    æ¨¡åž‹ç»“æž„è¦æ±‚æœ‰å›ºå®šæ•°é‡çš„ç›¸æœºè¾“å…¥ï¼Œä½†æ•°æ®é›†å¯èƒ½æ²¡æœ‰é‚£ä¹ˆå¤šç›¸æœº â†’ ç”¨â€œç©ºç›¸æœºâ€å ä½ã€‚
    å¦‚æžœè¾“å…¥é‡Œæ²¡æœ‰ STATEï¼Œå°±è¡¥ä¸€ä¸ªé»˜è®¤çš„ STATE feature
    å¦‚æžœè¾“å‡ºé‡Œæ²¡æœ‰ ACTIONï¼Œå°±è¡¥ä¸€ä¸ªé»˜è®¤çš„ ACTION feature
    '''
    def validate_features(self) -> None:
        for i in range(self.empty_cameras):
            key = OBS_IMAGES + f".empty_camera_{i}"
            empty_camera = PolicyFeature(
                type=FeatureType.VISUAL,
                shape=(3, *self.image_resolution).
            )
            self.input_features[key] = empty_camera

        if OBS_STATE not in self.input_features:
            state_feature = PolicyFeature(
                type=FeatureType.STATE,
                shape=(self.max_state_dim,),
            )
            self.input_features[OBS_STATE] = state_feature
        
        if ACTION not in self.output_features:
            action_feature = PolicyFeature(
                type=FeatureType.ACTION,
                shape=(self.max_action_dim,),
            )
            self.output_features[ACTION] = action_feature
    
    def get_optimizer_preset(self) -> AdamWConfig:
        return AdamWConfig(
            lr=self.optimizer_lr,
            betas=self.optimizer_betas,
            eps=self.optimizer_eps,
            weight_decay=self.optimizer_weight_decay,
            grad_clip_norm=self.optimizer_grad_clip_norm,
        )
    
    def get_scheduler_preset(self):
        return ConsineDecayWithWarmSchedulerConfig(
            peak_lr=self.optimizer_lr,
            decay_lr=self.scheduler_decay_lr,
            num_warmup_steps=self.scheduler_warmup_steps,
            num_decay_steps=self.scheduler_decay_steps,
        )
    
    '''
    @propertyæŠŠæ–¹æ³•ä¼ªè£…æˆå±žæ€§æ¥ä½¿ç”¨

    class A:
    def action_dim(self):
        return 50
    a = A()
    a.action_dim() å¿…é¡»åŠ æ‹¬å·

    class A:
        @property
        def action_dim(self):
            return 50
    a = A()
    a.action_dim ä¸ç”¨æ‹¬å·ï¼Œçœ‹èµ·æ¥åƒå˜é‡
    '''

    # è§‚æµ‹ç»™åŽŸå§‹å€¼ï¼Œä¸æ˜¯å¢žå¼ºå€¼
    @property
    def observation_delta_indices(self) -> None:
        return None
    
    # è¾“å‡ºçš„åŠ¨ä½œä¸æ˜¯ç»å¯¹åŠ¨ä½œï¼Œè€Œæ˜¯deltaå½¢å¼
    @property
    def action_delta_indices(self) -> list:
        return list(range(self.chunk_size))
    
    # reward ä¸ç”¨ delta(å¥–åŠ±å°±æ˜¯åŽŸå§‹æ ‡é‡åºåˆ—ï¼Œä¸åšå·®åˆ†ï¼‰
    @property
    def reward_delta_indices(self) -> None:
        return None